{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "sys.version_info(major=3, minor=7, micro=1, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. preprocessing\n",
    "# 2.build model\n",
    "## 2.1 encoder\n",
    "## 2.2 attention\n",
    "## 2.3 decoder\n",
    "#3. evaluation\n",
    "## 3.1 given sentence return\n",
    "## 3.2 visulize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Quien corrio?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_spa_file_path = \"input/spa.txt\"\n",
    "\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "unicode_to_ascii(\"¿Quién corrió?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> ¿ quien corrio ? <end>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    s = re.sub(r'[^a-zA-Z?.!,¿]', \" \", s)\n",
    "    \n",
    "    s= s.rstrip().strip()\n",
    "    \n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s\n",
    "\n",
    "preprocess_sentence(\"¿Quién corrió?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(filename):\n",
    "    lines = open(filename, encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
    "    sentence_pairs = [line.split(\"\\t\") for line in lines]\n",
    "    preprocessed_sentence_pairs = [\n",
    "        (preprocess_sentence(en), preprocess_sentence(sp)) for en, sp in sentence_pairs\n",
    "    ]\n",
    "    return zip(*preprocessed_sentence_pairs)\n",
    "\n",
    "en_dataset, sp_dataset = parse_data(en_spa_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 11\n"
     ]
    }
   ],
   "source": [
    "## id\n",
    "def tokenizer(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(num_words=None, filters=\"\", split=\" \")\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
    "\n",
    "def max_length(tensor):return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_input = max_length(input_tensor)\n",
    "max_length_output = max_length(output_tensor)\n",
    "print(max_length_input, max_length_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(example, tokenizer):\n",
    "    for t in example:\n",
    "        if t != 0:\n",
    "            print(\"%d --> %s\"%(t, tokenizer.index_word[t]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dateset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(30000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "train_dataset = make_dateset(input_train, output_train, batch_size, epochs, True)\n",
    "eval_dataset = make_dateset(input_eval, output_eval, batch_size, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_units = 256\n",
    "units = 1024\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_output.shape:  (64, 16, 1024)\n",
      "sample_hidden.shape:  (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))\n",
    "\n",
    "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "\n",
    "sample_output, sample_hidden = encoder(x, sample_hidden)\n",
    "\n",
    "print(\"sample_output.shape: \", sample_output.shape)\n",
    "print(\"sample_hidden.shape: \", sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention results.shape: (64, 1024)\n",
      "attention weights.shape: (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class BahanauAttention(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahanauAttention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, decoder_hidden, encoder_outputs):\n",
    "        # deocoder_hidden.shape:(batch_size, units)\n",
    "        # encoder_output.shape: (batch_size, length, units)\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "        \n",
    "        # before: (batch_size, length, units)\n",
    "        # after:  (batch_size, length, 1)\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n",
    "        \n",
    "        # shape: (batch_size, length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        # context_vector.shape: (batch_size, length, units)\n",
    "        context_vector = attention_weights * encoder_outputs\n",
    "        \n",
    "        #context_vector.shape: (batch, units) \n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "\n",
    "attention_model = BahanauAttention(units = 10)\n",
    "attention_results, attention_weights = attention_model(sample_hidden, sample_output)\n",
    "\n",
    "print(\"attention results.shape:\", attention_results.shape)\n",
    "print(\"attention weights.shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_output shape: (64, 4935)\n",
      "decoder_hidden shape: (64, 1024)\n",
      "decoder_attention_weights.shape: (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, \n",
    "                 decoding_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru= keras.layers.GRU(self.decoding_units, \n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahanauAttention(self.decoding_units)\n",
    "        \n",
    "        \n",
    "    def call(self, x, hidden, encoding_outputs):\n",
    "        # context_vecot.shape: (batch_size, units)\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            hidden, encoding_outputs\n",
    "         )\n",
    "        # before embedding: x.shape: (batch_size, 1)\n",
    "        \n",
    "        # after embdding: x.shape: (batch_size, 1, embedding_units)\n",
    "        x = self.embedding(x)\n",
    "        combined_x = tf.concat(\n",
    "            [tf.expand_dims(context_vector, 1), x], axis = -1)\n",
    "        \n",
    "        output, state = self.gru(combined_x)\n",
    "        \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output.shape [batch_size, vocab_size]\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output, state, attention_weights\n",
    "    \n",
    "decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n",
    "outputs = decoder(tf.random.uniform((batch_size, 1)), sample_hidden, sample_output)\n",
    "\n",
    "decoder_output, decoder_hidden, decoder_aw = outputs\n",
    "\n",
    "print(\"decoder_output shape:\", decoder_output.shape)\n",
    "print(\"decoder_hidden shape:\", decoder_hidden.shape)\n",
    "print(\"decoder_attention_weights.shape:\", decoder_aw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, encoding_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs, encoding_hidden = encoder(inp, encoding_hidden)\n",
    "        decoding_hidden = encoding_hidden\n",
    "        \n",
    "        # eg: <start> I am here <end>\n",
    "        # 1. <start> -> I\n",
    "        # 4. here -> <end>\n",
    "        for t in range(0, targ.shape[1] - 1):\n",
    "            decoding_input = tf.expand_dims(targ[:, t], 1)\n",
    "            predictions, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_outputs)\n",
    "            loss += loss_function(targ[:,t+1], predictions)\n",
    "            \n",
    "    batch_loss = loss/int(targ.shape[0])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables # list +\n",
    "    \n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.7744\n",
      "Epoch 1 Batch 100 Loss 0.3883\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = len(input_tensor) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    encoding_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, encoding_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "\n",
    "    print(\"Epoch {} Loss {:.4f}\".format(epoch +1, total_loss/steps_per_epoch))\n",
    "    \n",
    "    print(\"Time take for 1 epoch {} sec\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_sentence):\n",
    "    attention_matrix = np.zeros((max_length_output, max_length_input))\n",
    "    input_sentence = preprocess_sentence(input_sentence)\n",
    "    \n",
    "    inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(\" \")]\n",
    "    \n",
    "    inputs = keras.preprocessing.sequence.pad_sequences([inputs], maxlen= max_length_input, padding=\"post\")\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    results = \"\"\n",
    "#     encoding_hidden = encoder.initialize_hidden_state()\n",
    "    encoding_hidden = tf.zeros((1, units))\n",
    "    \n",
    "    encoding_outputs, encoding_hidden = encoder(inputs, encoding_hidden)\n",
    "    \n",
    "    decoding_hidden = encoding_hidden\n",
    "    \n",
    "    # eg: <start> -> A\n",
    "    # A -> B -> C -> D\n",
    "    \n",
    "    decoding_input = tf.expand_dims(\n",
    "        [output_tokenizer.word_index[\"<start>\"]], 0\n",
    "    )\n",
    "    \n",
    "    for t in range(max_length_output):\n",
    "        predictions, decoding_hidden, attention_weights = decoder(decoding_input, decoding_hidden, encoding_outputs)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        \n",
    "        attention_matrix[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        results += output_tokenizer.index_word[predicted_id] + \" \"\n",
    "        \n",
    "        if output_tokenizer.index_word[predicted_id] == \"<end>\":\n",
    "            return results, input_sentence, attention_matrix\n",
    "        \n",
    "        decoding_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    return results, input_sentence, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.matshow(attention_matrix, cmap = \"viridis\")\n",
    "    \n",
    "    font_dict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([\"\"] + input_sentence, fontdict = font_dict, rotation = 90)\n",
    "    ax.set_yticklabels([\"\"] + predicted_sentence, fontdict = font_dict)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def translate(input_sentence):\n",
    "    results, input_sentence, attention_matrix = evaluate(input_sentence)\n",
    "    print(\"Input: %s\"% (input_sentence))\n",
    "    print(\"Predicted translation: %s\" % (results))\n",
    "    \n",
    "    attention_matrix = attention_matrix[:len(results.split(\" \")), :len(input_sentence.split(\" \"))]\n",
    "    \n",
    "    plot_attention(attention_matrix, input_sentence.split(\" \"), results.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui <end>\n",
      "Predicted translation: <start> it s very hot !  end <end>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u\"hace mucho frío aquí\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
