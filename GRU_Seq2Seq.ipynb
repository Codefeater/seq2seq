{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "sys.version_info(major=3, minor=7, micro=1, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. preprocessing\n",
    "# 2.build model\n",
    "## 2.1 encoder\n",
    "## 2.2 attention\n",
    "## 2.3 decoder\n",
    "#3. evaluation\n",
    "## 3.1 given sentence return\n",
    "## 3.2 visulize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Quien corrio?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_spa_file_path = \"input/spa.txt\"\n",
    "\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "unicode_to_ascii(\"¿Quién corrió?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> ¿ quien corrio ? <end>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    s = re.sub(r'[^a-zA-Z?.!,¿]', \" \", s)\n",
    "    \n",
    "    s= s.rstrip().strip()\n",
    "    \n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s\n",
    "\n",
    "preprocess_sentence(\"¿Quién corrió?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(filename):\n",
    "    lines = open(filename, encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
    "    sentence_pairs = [line.split(\"\\t\") for line in lines]\n",
    "    preprocessed_sentence_pairs = [\n",
    "        (preprocess_sentence(en), preprocess_sentence(sp)) for en, sp in sentence_pairs\n",
    "    ]\n",
    "    return zip(*preprocessed_sentence_pairs)\n",
    "\n",
    "en_dataset, sp_dataset = parse_data(en_spa_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 11\n"
     ]
    }
   ],
   "source": [
    "## id\n",
    "def tokenizer(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(num_words=None, filters=\"\", split=\" \")\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
    "\n",
    "def max_length(tensor):return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_input = max_length(input_tensor)\n",
    "max_length_output = max_length(output_tensor)\n",
    "print(max_length_input, max_length_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(example, tokenizer):\n",
    "    for t in example:\n",
    "        if t != 0:\n",
    "            print(\"%d --> %s\"%(t, tokenizer.index_word[t]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dateset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(30000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "train_dataset = make_dateset(input_train, output_train, batch_size, epochs, True)\n",
    "eval_dataset = make_dateset(input_eval, output_eval, batch_size, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_units = 128\n",
    "units = 512\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_output.shape:  (64, 16, 512)\n",
      "sample_hidden.shape:  (64, 512)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))\n",
    "\n",
    "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "\n",
    "sample_output, sample_hidden = encoder(x, sample_hidden)\n",
    "\n",
    "print(\"sample_output.shape: \", sample_output.shape)\n",
    "print(\"sample_hidden.shape: \", sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention results.shape: (64, 512)\n",
      "attention weights.shape: (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class BahanauAttention(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahanauAttention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, decoder_hidden, encoder_outputs):\n",
    "        # deocoder_hidden.shape:(batch_size, units)\n",
    "        # encoder_output.shape: (batch_size, length, units)\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "        \n",
    "        # before: (batch_size, length, units)\n",
    "        # after:  (batch_size, length, 1)\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n",
    "        \n",
    "        # shape: (batch_size, length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        # context_vector.shape: (batch_size, length, units)\n",
    "        context_vector = attention_weights * encoder_outputs\n",
    "        \n",
    "        #context_vector.shape: (batch, units) \n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "\n",
    "attention_model = BahanauAttention(units = 10)\n",
    "attention_results, attention_weights = attention_model(sample_hidden, sample_output)\n",
    "\n",
    "print(\"attention results.shape:\", attention_results.shape)\n",
    "print(\"attention weights.shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_output shape: (64, 4935)\n",
      "decoder_hidden shape: (64, 512)\n",
      "decoder_attention_weights.shape: (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, \n",
    "                 decoding_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru= keras.layers.GRU(self.decoding_units, \n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahanauAttention(self.decoding_units)\n",
    "        \n",
    "        \n",
    "    def call(self, x, hidden, encoding_outputs):\n",
    "        # context_vecot.shape: (batch_size, units)\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            hidden, encoding_outputs\n",
    "         )\n",
    "        # before embedding: x.shape: (batch_size, 1)\n",
    "        \n",
    "        # after embdding: x.shape: (batch_size, 1, embedding_units)\n",
    "        x = self.embedding(x)\n",
    "        combined_x = tf.concat(\n",
    "            [tf.expand_dims(context_vector, 1), x], axis = -1)\n",
    "        \n",
    "        output, state = self.gru(combined_x)\n",
    "        \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output.shape [batch_size, vocab_size]\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output, state, attention_weights\n",
    "    \n",
    "decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n",
    "outputs = decoder(tf.random.uniform((batch_size, 1)), sample_hidden, sample_output)\n",
    "\n",
    "decoder_output, decoder_hidden, decoder_aw = outputs\n",
    "\n",
    "print(\"decoder_output shape:\", decoder_output.shape)\n",
    "print(\"decoder_hidden shape:\", decoder_hidden.shape)\n",
    "print(\"decoder_attention_weights.shape:\", decoder_aw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, encoding_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs, encoding_hidden = encoder(inp, encoding_hidden)\n",
    "        decoding_hidden = encoding_hidden\n",
    "        \n",
    "        # eg: <start> I am here <end>\n",
    "        # 1. <start> -> I\n",
    "        # 4. here -> <end>\n",
    "        for t in range(0, targ.shape[1] - 1):\n",
    "            decoding_input = tf.expand_dims(targ[:, t], 1)\n",
    "            predictions, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_outputs)\n",
    "            loss += loss_function(targ[:,t+1], predictions)\n",
    "            \n",
    "    batch_loss = loss/int(targ.shape[0])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables # list +\n",
    "    \n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.8139\n",
      "Epoch 1 Batch 100 Loss 0.3915\n",
      "Epoch 1 Batch 200 Loss 0.3571\n",
      "Epoch 1 Batch 300 Loss 0.3353\n",
      "Epoch 1 Batch 400 Loss 0.3007\n",
      "Epoch 1 Loss 0.3754\n",
      "Time take for 1 epoch 246.4912497997284 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.3026\n",
      "Epoch 2 Batch 100 Loss 0.2631\n",
      "Epoch 2 Batch 200 Loss 0.2680\n",
      "Epoch 2 Batch 300 Loss 0.2210\n",
      "Epoch 2 Batch 400 Loss 0.2396\n",
      "Epoch 2 Loss 0.2652\n",
      "Time take for 1 epoch 226.71780467033386 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2259\n",
      "Epoch 3 Batch 100 Loss 0.2166\n",
      "Epoch 3 Batch 200 Loss 0.2078\n",
      "Epoch 3 Batch 300 Loss 0.2090\n",
      "Epoch 3 Batch 400 Loss 0.1782\n",
      "Epoch 3 Loss 0.2038\n",
      "Time take for 1 epoch 229.55093097686768 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1629\n",
      "Epoch 4 Batch 100 Loss 0.1695\n",
      "Epoch 4 Batch 200 Loss 0.1796\n",
      "Epoch 4 Batch 300 Loss 0.1665\n",
      "Epoch 4 Batch 400 Loss 0.1259\n",
      "Epoch 4 Loss 0.1568\n",
      "Time take for 1 epoch 229.23765397071838 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1131\n",
      "Epoch 5 Batch 100 Loss 0.1216\n",
      "Epoch 5 Batch 200 Loss 0.1184\n",
      "Epoch 5 Batch 300 Loss 0.1294\n",
      "Epoch 5 Batch 400 Loss 0.1007\n",
      "Epoch 5 Loss 0.1188\n",
      "Time take for 1 epoch 232.18633389472961 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = len(input_tensor) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    encoding_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, encoding_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "\n",
    "    print(\"Epoch {} Loss {:.4f}\".format(epoch +1, total_loss/steps_per_epoch))\n",
    "    \n",
    "    print(\"Time take for 1 epoch {} sec\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_sentence):\n",
    "    attention_matrix = np.zeros((max_length_output, max_length_input))\n",
    "    input_sentence = preprocess_sentence(input_sentence)\n",
    "    \n",
    "    inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(\" \")]\n",
    "    \n",
    "    inputs = keras.preprocessing.sequence.pad_sequences([inputs], maxlen= max_length_input, padding=\"post\")\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    results = \"\"\n",
    "#     encoding_hidden = encoder.initialize_hidden_state()\n",
    "    encoding_hidden = tf.zeros((1, units))\n",
    "    \n",
    "    encoding_outputs, encoding_hidden = encoder(inputs, encoding_hidden)\n",
    "    \n",
    "    decoding_hidden = encoding_hidden\n",
    "    \n",
    "    # eg: <start> -> A\n",
    "    # A -> B -> C -> D\n",
    "    \n",
    "    decoding_input = tf.expand_dims(\n",
    "        [output_tokenizer.word_index[\"<start>\"]], 0\n",
    "    )\n",
    "    \n",
    "    for t in range(max_length_output):\n",
    "        predictions, decoding_hidden, attention_weights = decoder(decoding_input, decoding_hidden, encoding_outputs)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        \n",
    "        attention_matrix[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        results += output_tokenizer.index_word[predicted_id] + \" \"\n",
    "        \n",
    "        if output_tokenizer.index_word[predicted_id] == \"<end>\":\n",
    "            return results, input_sentence, attention_matrix\n",
    "        \n",
    "        decoding_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    return results, input_sentence, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.matshow(attention_matrix, cmap = \"viridis\")\n",
    "    \n",
    "    font_dict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([\"\"] + input_sentence, fontdict = font_dict, rotation = 90)\n",
    "    ax.set_yticklabels([\"\"] + predicted_sentence, fontdict = font_dict)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def translate(input_sentence):\n",
    "    results, input_sentence, attention_matrix = evaluate(input_sentence)\n",
    "    print(\"Input: %s\"% (input_sentence))\n",
    "    print(\"Predicted translation: %s\" % (results))\n",
    "    \n",
    "    attention_matrix = attention_matrix[:len(results.split(\" \")), :len(input_sentence.split(\" \"))]\n",
    "    \n",
    "    plot_attention(attention_matrix, input_sentence.split(\" \"), results.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui <end>\n",
      "Predicted translation: it s very hot here . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u\"hace mucho frío aquí\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
