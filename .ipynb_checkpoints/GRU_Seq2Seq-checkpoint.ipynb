{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "sys.version_info(major=3, minor=7, micro=1, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. preprocessing\n",
    "# 2.build model\n",
    "## 2.1 encoder\n",
    "## 2.2 attention\n",
    "## 2.3 decoder\n",
    "#3. evaluation\n",
    "## 3.1 given sentence return\n",
    "## 3.2 visulize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Quien corrio?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_spa_file_path = \"input/spa.txt\"\n",
    "\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "unicode_to_ascii(\"¿Quién corrió?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> ¿ quien corrio ? <end>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    s = re.sub(r'[^a-zA-Z?.!,¿]', \" \", s)\n",
    "    \n",
    "    s= s.rstrip().strip()\n",
    "    \n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s\n",
    "\n",
    "preprocess_sentence(\"¿Quién corrió?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(filename):\n",
    "    lines = open(filename, encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
    "    sentence_pairs = [line.split(\"\\t\") for line in lines]\n",
    "    preprocessed_sentence_pairs = [\n",
    "        (preprocess_sentence(en), preprocess_sentence(sp)) for en, sp in sentence_pairs\n",
    "    ]\n",
    "    return zip(*preprocessed_sentence_pairs)\n",
    "\n",
    "en_dataset, sp_dataset = parse_data(en_spa_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 11\n"
     ]
    }
   ],
   "source": [
    "## id\n",
    "def tokenizer(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(num_words=None, filters=\"\", split=\" \")\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
    "\n",
    "def max_length(tensor):return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_input = max_length(input_tensor)\n",
    "max_length_output = max_length(output_tensor)\n",
    "print(max_length_input, max_length_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(example, tokenizer):\n",
    "    for t in example:\n",
    "        if t != 0:\n",
    "            print(\"%d --> %s\"%(t, tokenizer.index_word[t]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dateset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(30000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "train_dataset = make_dateset(input_train, output_train, batch_size, epochs, True)\n",
    "eval_dataset = make_dateset(input_eval, output_eval, batch_size, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_units = 128\n",
    "units = 512\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_output.shape:  (64, 16, 512)\n",
      "sample_hidden.shape:  (64, 512)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))\n",
    "\n",
    "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "\n",
    "sample_output, sample_hidden = encoder(x, sample_hidden)\n",
    "\n",
    "print(\"sample_output.shape: \", sample_output.shape)\n",
    "print(\"sample_hidden.shape: \", sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention results.shape: (64, 512)\n",
      "attention weights.shape: (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class BahanauAttention(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahanauAttention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, decoder_hidden, encoder_outputs):\n",
    "        # deocoder_hidden.shape:(batch_size, units)\n",
    "        # encoder_output.shape: (batch_size, length, units)\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "        \n",
    "        # before: (batch_size, length, units)\n",
    "        # after:  (batch_size, length, 1)\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n",
    "        \n",
    "        # shape: (batch_size, length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        # context_vector.shape: (batch_size, length, units)\n",
    "        context_vector = attention_weights * encoder_outputs\n",
    "        \n",
    "        #context_vector.shape: (batch, units) \n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "\n",
    "attention_model = BahanauAttention(units = 10)\n",
    "attention_results, attention_weights = attention_model(sample_hidden, sample_output)\n",
    "\n",
    "print(\"attention results.shape:\", attention_results.shape)\n",
    "print(\"attention weights.shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_output shape: (64, 4935)\n",
      "decoder_hidden shape: (64, 512)\n",
      "decoder_attention_weights.shape: (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, \n",
    "                 decoding_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru= keras.layers.GRU(self.decoding_units, \n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahanauAttention(self.decoding_units)\n",
    "        \n",
    "        \n",
    "    def call(self, x, hidden, encoding_outputs):\n",
    "        # context_vecot.shape: (batch_size, units)\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            hidden, encoding_outputs\n",
    "         )\n",
    "        # before embedding: x.shape: (batch_size, 1)\n",
    "        \n",
    "        # after embdding: x.shape: (batch_size, 1, embedding_units)\n",
    "        x = self.embedding(x)\n",
    "        combined_x = tf.concat(\n",
    "            [tf.expand_dims(context_vector, 1), x], axis = -1)\n",
    "        \n",
    "        output, state = self.gru(combined_x)\n",
    "        \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output.shape [batch_size, vocab_size]\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output, state, attention_weights\n",
    "    \n",
    "decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n",
    "outputs = decoder(tf.random.uniform((batch_size, 1)), sample_hidden, sample_output)\n",
    "\n",
    "decoder_output, decoder_hidden, decoder_aw = outputs\n",
    "\n",
    "print(\"decoder_output shape:\", decoder_output.shape)\n",
    "print(\"decoder_hidden shape:\", decoder_hidden.shape)\n",
    "print(\"decoder_attention_weights.shape:\", decoder_aw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, encoding_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs, encoding_hidden = encoder(inp, encoding_hidden)\n",
    "        decoding_hidden = encoding_hidden\n",
    "        \n",
    "        # eg: <start> I am here <end>\n",
    "        # 1. <start> -> I\n",
    "        # 4. here -> <end>\n",
    "        for t in range(0, targ.shape[1] - 1):\n",
    "            decoding_input = tf.expand_dims(targ[:, t], 1)\n",
    "            predictions, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_outputs)\n",
    "            loss += loss_function(targ[:,t+1], predictions)\n",
    "            \n",
    "    batch_loss = loss/int(targ.shape[0])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables # list +\n",
    "    \n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.7806\n",
      "Epoch 1 Batch 100 Loss 0.4029\n",
      "Epoch 1 Batch 200 Loss 0.3665\n",
      "Epoch 1 Batch 300 Loss 0.3438\n",
      "Epoch 1 Batch 400 Loss 0.3149\n",
      "Epoch 1 Loss 0.3792\n",
      "Time take for 1 epoch 287.146733045578 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2988\n",
      "Epoch 2 Batch 100 Loss 0.3036\n",
      "Epoch 2 Batch 200 Loss 0.2825\n",
      "Epoch 2 Batch 300 Loss 0.2710\n",
      "Epoch 2 Batch 400 Loss 0.2465\n",
      "Epoch 2 Loss 0.2791\n",
      "Time take for 1 epoch 284.77845287323 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2469\n",
      "Epoch 3 Batch 100 Loss 0.2504\n",
      "Epoch 3 Batch 200 Loss 0.2298\n",
      "Epoch 3 Batch 300 Loss 0.2225\n",
      "Epoch 3 Batch 400 Loss 0.1974\n",
      "Epoch 3 Loss 0.2289\n",
      "Time take for 1 epoch 308.0652632713318 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1914\n",
      "Epoch 4 Batch 100 Loss 0.1883\n",
      "Epoch 4 Batch 200 Loss 0.1941\n",
      "Epoch 4 Batch 300 Loss 0.1846\n",
      "Epoch 4 Batch 400 Loss 0.1443\n",
      "Epoch 4 Loss 0.1819\n",
      "Time take for 1 epoch 298.3591899871826 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1435\n",
      "Epoch 5 Batch 100 Loss 0.1424\n",
      "Epoch 5 Batch 200 Loss 0.1583\n",
      "Epoch 5 Batch 300 Loss 0.1529\n",
      "Epoch 5 Batch 400 Loss 0.1209\n",
      "Epoch 5 Loss 0.1409\n",
      "Time take for 1 epoch 306.2422299385071 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = len(input_tensor) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    encoding_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, encoding_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "\n",
    "    print(\"Epoch {} Loss {:.4f}\".format(epoch +1, total_loss/steps_per_epoch))\n",
    "    \n",
    "    print(\"Time take for 1 epoch {} sec\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_sentence):\n",
    "    attention_matrix = np.zeros((max_length_output, max_length_input))\n",
    "    input_sentence = preprocess_sentence(input_sentence)\n",
    "    \n",
    "    inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(\" \")]\n",
    "    \n",
    "    inputs = keras.preprocessing.sequence.pad_sequences([inputs], maxlen= max_length_input, padding=\"post\")\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    results = \"\"\n",
    "#     encoding_hidden = encoder.initialize_hidden_state()\n",
    "    encoding_hidden = tf.zeros((1, units))\n",
    "    \n",
    "    encoding_outputs, encoding_hidden = encoder(inputs, encoding_hidden)\n",
    "    \n",
    "    decoding_hidden = encoding_hidden\n",
    "    \n",
    "    # eg: <start> -> A\n",
    "    # A -> B -> C -> D\n",
    "    \n",
    "    decoding_input = tf.expand_dims(\n",
    "        [output_tokenizer.word_index[\"<start>\"]], 0\n",
    "    )\n",
    "    \n",
    "    for t in range(max_length_output):\n",
    "        predictions, decoding_hidden, attention_weights = decoder(decoding_input, decoding_hidden, encoding_outputs)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        \n",
    "        attention_matrix[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        results += output_tokenizer.index_word[predicted_id] + \" \"\n",
    "        \n",
    "        if output_tokenizer.index_word[predicted_id] == \"<end>\":\n",
    "            return results, input_sentence, attention_matrix\n",
    "        \n",
    "        decoding_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    return results, input_sentence, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.matshow(attention_matrix, cmap = \"viridis\")\n",
    "    \n",
    "    font_dict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([\"\"] + input_sentence, fontdict = font_dict, rotation = 90)\n",
    "    ax.set_yticklabels([\"\"] + predicted_sentence, fontdict = font_dict)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def translate(input_sentence):\n",
    "    results, input_sentence, attention_matrix = evaluate(input_sentence)\n",
    "    print(\"Input: %s\"% (input_sentence))\n",
    "    print(\"Predicted translation: %s\" % (results))\n",
    "    \n",
    "    attention_matrix = attention_matrix[:len(results.split(\" \")), :len(input_sentence.split(\" \"))]\n",
    "    \n",
    "    plot_attention(attention_matrix, input_sentence.split(\" \"), results.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui <end>\n",
      "Predicted translation: it s very cold . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u\"hace mucho frío aquí\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
